# 3.2 决策树算法

## 3.2.1 决策树概念
决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法

决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树。

怎么理解这句话？通过一个对话例子

![图3-2-1](../imgs/3-2-1.png)

想一想这个女生为什么把年龄放在最上面判断！

上面案例是女生通过定性的主观意识，把年龄放到最上面，那么如果需要对这一过程进行量化，该如何处理呢？

此时需要用到信息论中的知识：信息熵，信息增益


## 3.2.2 决策树分类原理

### 1. 熵概念
物理学上，熵 Entropy 是“混乱”程度的量度。

系统越有序，熵值越低；系统越混乱或者分散，熵值越高。

1. 从信息完整性描述

当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。

2. 从信息有序性描述

当数据量一致时，系统越有序，熵值越低；系统越混乱或者分散，熵值越高。

假如事件A的分类划分是（A1,A2,...,An），每部分发生的概率是(p1,p2,...,pn)，那信息熵定义为公式如下：（log是以2为底）

![图3-2-2](../imgs/3-2-2.png)

### 2. 熵案例介绍

1. 如果一颗骰子的六个面都是1 ，投掷它不会给你带来任何新信息，因为你知道它的结果肯定是1，它的信息熵为？

答案：log(1) = 0

2. 假设我们没有看世界杯的比赛，但是想知道哪支球队会是冠军，我们只能猜测某支球队是或不是冠军，然后观众用对或不对来回答，我们想要猜测次数尽可能少，你会用什么方法？

答案：二分法  
计算过程：  
假如有 16 支球队，分别编号，先问是否在 1-8 之间，如果是就继续问是否在 1-4 之间，  
以此类推，直到最后判断出冠军球队是哪只。  
如果球队数量是 16，我们需要问 4 次来得到最后的答案。那么世界冠军这条消息的信息熵就是 4。

如果有32个球队，准确的信息量应该是：   
H = -（p1 * logp1 + p2 * logp2 + ... + p32 * logp32），  
其中 p1, ..., p32 分别是这 32 支球队夺冠的概率。  
当每支球队夺冠概率相等都是 1/32 的时：H = -（32 * 1/32 * log1/32） = 5  
每个事件概率相同时，熵最大，这件事越不确定。

### 3. 信息增益概念
以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。

特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：

![图3-2-3](../imgs/3-2-3.png)


### 3. 信息增益案例
如下左图，第一列为论坛号码，第二列为性别，第三列为活跃度，最后一列用户是否流失。

我们要解决一个问题：性别和活跃度两个特征，哪个对用户流失影响更大？

![图3-2-4](../imgs/3-2-4.png)

通过计算信息增益可以解决这个问题，统计上右表信息

其中Positive为正样本（已流失），Negative为负样本（未流失），下面的数值为不同划分下对应的人数。

可得到三个熵：

整体熵：

![图3-2-5](../imgs/3-2-5.png)

性别熵：

![图3-2-6](../imgs/3-2-6.png)

活跃度熵：

![图3-2-7](../imgs/3-2-7.png)


计算信息增益看指标影响：

性别信息增益：

![图3-2-8](../imgs/3-2-8.png)

活跃度信息增益：

![图3-2-9](../imgs/3-2-9.png)

活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。

在做特征选择或者数据分析的时候，我们应该重点考察活跃度这个指标。