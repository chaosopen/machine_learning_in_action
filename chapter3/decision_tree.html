<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>3.2 决策树算法 | 《人工智能之机器学习入门到实战》电子书</title>
    <meta name="generator" content="VuePress 1.9.10">
    <script>var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?ad853f2bf7970012985d6598765c2f9a";
          var s = document.getElementsByTagName("script")[0]; 
          s.parentNode.insertBefore(hm, s);
        })();</script>
    <meta name="description" content="">
    <meta name="keywords" content="人工智能,机器学习,数据分析">
    
    <link rel="preload" href="/assets/css/0.styles.fc446acf.css" as="style"><link rel="preload" href="/assets/js/app.d8974754.js" as="script"><link rel="preload" href="/assets/js/4.c4ff76d6.js" as="script"><link rel="preload" href="/assets/js/1.bfbdf300.js" as="script"><link rel="preload" href="/assets/js/11.a789a3a6.js" as="script"><link rel="prefetch" href="/assets/js/12.2c0cace7.js"><link rel="prefetch" href="/assets/js/13.2b46b2e1.js"><link rel="prefetch" href="/assets/js/14.646dcb04.js"><link rel="prefetch" href="/assets/js/15.e89d864b.js"><link rel="prefetch" href="/assets/js/16.ec274341.js"><link rel="prefetch" href="/assets/js/17.0839d42f.js"><link rel="prefetch" href="/assets/js/18.954c84db.js"><link rel="prefetch" href="/assets/js/19.b518d285.js"><link rel="prefetch" href="/assets/js/2.5a23befa.js"><link rel="prefetch" href="/assets/js/20.53970987.js"><link rel="prefetch" href="/assets/js/21.2338aa9c.js"><link rel="prefetch" href="/assets/js/22.30fc9cde.js"><link rel="prefetch" href="/assets/js/23.978ae11f.js"><link rel="prefetch" href="/assets/js/24.0e445b2e.js"><link rel="prefetch" href="/assets/js/25.09982af2.js"><link rel="prefetch" href="/assets/js/26.a4ef7a52.js"><link rel="prefetch" href="/assets/js/27.f1d4807a.js"><link rel="prefetch" href="/assets/js/28.e371a649.js"><link rel="prefetch" href="/assets/js/29.0fd1dd7b.js"><link rel="prefetch" href="/assets/js/3.75fc72be.js"><link rel="prefetch" href="/assets/js/30.773fd89c.js"><link rel="prefetch" href="/assets/js/31.6076da38.js"><link rel="prefetch" href="/assets/js/32.f825479a.js"><link rel="prefetch" href="/assets/js/33.a29f3ceb.js"><link rel="prefetch" href="/assets/js/34.f2af317e.js"><link rel="prefetch" href="/assets/js/35.511501a7.js"><link rel="prefetch" href="/assets/js/36.8648a2d4.js"><link rel="prefetch" href="/assets/js/37.316eadb4.js"><link rel="prefetch" href="/assets/js/38.499d1579.js"><link rel="prefetch" href="/assets/js/39.7a208fe4.js"><link rel="prefetch" href="/assets/js/40.8d17af82.js"><link rel="prefetch" href="/assets/js/41.6c93ae02.js"><link rel="prefetch" href="/assets/js/42.e4d6db78.js"><link rel="prefetch" href="/assets/js/43.84b121d5.js"><link rel="prefetch" href="/assets/js/44.1aa053e8.js"><link rel="prefetch" href="/assets/js/45.3a4e7f3e.js"><link rel="prefetch" href="/assets/js/46.e4225b3b.js"><link rel="prefetch" href="/assets/js/47.1aa4acbd.js"><link rel="prefetch" href="/assets/js/48.26f8e6df.js"><link rel="prefetch" href="/assets/js/49.eef41bc0.js"><link rel="prefetch" href="/assets/js/5.c1285393.js"><link rel="prefetch" href="/assets/js/50.a05f3f51.js"><link rel="prefetch" href="/assets/js/51.83251fba.js"><link rel="prefetch" href="/assets/js/52.1b3cebd4.js"><link rel="prefetch" href="/assets/js/53.54dd6a71.js"><link rel="prefetch" href="/assets/js/54.651a7cd3.js"><link rel="prefetch" href="/assets/js/55.fb0937d5.js"><link rel="prefetch" href="/assets/js/56.23b34fb2.js"><link rel="prefetch" href="/assets/js/57.58454b35.js"><link rel="prefetch" href="/assets/js/58.d10028ff.js"><link rel="prefetch" href="/assets/js/59.aa56fec1.js"><link rel="prefetch" href="/assets/js/6.c05d10c1.js"><link rel="prefetch" href="/assets/js/7.a9232db4.js"><link rel="prefetch" href="/assets/js/8.7bceb903.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.6a75a8f0.js">
    <link rel="stylesheet" href="/assets/css/0.styles.fc446acf.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">《人工智能之机器学习入门到实战》电子书</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  主页
</a></div><div class="nav-item"><a href="/guide/" class="nav-link">
  一起交流
</a></div><div class="nav-item"><a href="/sponsor.html" class="nav-link">
  赞助
</a></div><div class="nav-item"><a href="https://github.com/chaosopen/machine_learning_in_action" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  主页
</a></div><div class="nav-item"><a href="/guide/" class="nav-link">
  一起交流
</a></div><div class="nav-item"><a href="/sponsor.html" class="nav-link">
  赞助
</a></div><div class="nav-item"><a href="https://github.com/chaosopen/machine_learning_in_action" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/" aria-current="page" class="sidebar-link">首页</a></li><li><section class="sidebar-group depth-0"><a href="/chapter1/index" class="sidebar-heading clickable"><span>第一章：人工智能入门</span> <!----></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/chapter1/ai_intro.html" class="sidebar-link">1.1 人工智能介绍</a></li><li><a href="/chapter1/ml_intro.html" class="sidebar-link">1.2 机器学习介绍</a></li></ul></section></li><li><section class="sidebar-group depth-0"><a href="/chapter2/index" class="sidebar-heading clickable"><span>第二章：机器学习基础</span> <!----></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/chapter2/ml_workflow.html" class="sidebar-link">2.1 机器学习工作流程</a></li><li><a href="/chapter2/ml_category.html" class="sidebar-link">2.2 机器学习算法分类</a></li><li><a href="/chapter2/model_intro.html" class="sidebar-link">2.3 模型介绍</a></li><li><a href="/chapter2/install_ml.html" class="sidebar-link">2.4 安装机器学习环境</a></li><li><a href="/chapter2/first_ml_project.html" class="sidebar-link">2.5 第一个机器学习项目</a></li></ul></section></li><li><section class="sidebar-group depth-0"><a href="/chapter3/index" class="sidebar-heading clickable open"><span>第三章：机器学习算法</span> <!----></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/chapter3/knn.html" class="sidebar-link">3.1 K-近邻算法介绍</a></li><li><a href="/chapter3/decision_tree.html" aria-current="page" class="active sidebar-link">3.2 决策树算法</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-2-1-决策树概念" class="sidebar-link">3.2.1 决策树概念</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-2-2-决策树分类原理" class="sidebar-link">3.2.2 决策树分类原理</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_1-熵概念" class="sidebar-link">1. 熵概念</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-熵案例介绍" class="sidebar-link">2. 熵案例介绍</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-信息增益概念" class="sidebar-link">3. 信息增益概念</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-信息增益案例" class="sidebar-link">3. 信息增益案例</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_4-信息增益率" class="sidebar-link">4. 信息增益率</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_5-基尼值和基尼指数" class="sidebar-link">5. 基尼值和基尼指数</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_6-基尼指数案例" class="sidebar-link">6. 基尼指数案例</a></li></ul></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-2-3-决策树构建总结" class="sidebar-link">3.2.3 决策树构建总结</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_1-构建决策树步骤" class="sidebar-link">1. 构建决策树步骤</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-决策树变量" class="sidebar-link">2. 决策树变量</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-评估分割点的好坏" class="sidebar-link">3. 评估分割点的好坏</a></li></ul></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-2-4-常见决策树类型比较" class="sidebar-link">3.2.4 常见决策树类型比较</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_1-id3-算法" class="sidebar-link">1. ID3 算法</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-c4-5算法" class="sidebar-link">2. C4.5算法</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-cart算法" class="sidebar-link">3. CART算法</a></li></ul></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_3-2-5-cart剪枝" class="sidebar-link">3.2.5 cart剪枝</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_1-为什么要剪枝" class="sidebar-link">1. 为什么要剪枝</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-常用的减枝方法" class="sidebar-link">2. 常用的减枝方法</a></li></ul></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-2-6-决策树算法api" class="sidebar-link">2.2.6 决策树算法API</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_1-构建参数介绍" class="sidebar-link">1. 构建参数介绍</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-代码过程" class="sidebar-link">2. 代码过程</a></li></ul></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-2-7-决策树可视化" class="sidebar-link">2.2.7 决策树可视化</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_1-加载模型" class="sidebar-link">1. 加载模型</a></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-生成可视化图片" class="sidebar-link">2. 生成可视化图片</a></li></ul></li><li class="sidebar-sub-header"><a href="/chapter3/decision_tree.html#_2-2-8-决策树算法优缺点" class="sidebar-link">2.2.8 决策树算法优缺点</a></li></ul></li><li><a href="/chapter3/naive_bayes.html" class="sidebar-link">3.3 朴素贝叶斯算法</a></li><li><a href="/chapter3/linear_regression.html" class="sidebar-link">3.4 线性回归算法</a></li><li><a href="/chapter3/logistic_regression.html" class="sidebar-link">3.5 逻辑回归算法</a></li><li><a href="/chapter3/svm.html" class="sidebar-link">3.6 SVM算法</a></li><li><a href="/chapter3/random_forest.html" class="sidebar-link">3.7 随机森林算法</a></li><li><a href="/chapter3/kmeans.html" class="sidebar-link">3.8 K-Means聚类算法</a></li><li><a href="/chapter3/algorithm_selection.html" class="sidebar-link">3.9 算法选择建议</a></li></ul></section></li><li><section class="sidebar-group depth-0"><a href="/chapter4/index" class="sidebar-heading clickable"><span>第四章：算法通用知识</span> <!----></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/chapter4/distance_measurement.html" class="sidebar-link">4.1 距离度量</a></li><li><a href="/chapter4/feature_engineering.html" class="sidebar-link">4.2 特征工程</a></li><li><a href="/chapter4/regression_evaluation.html" class="sidebar-link">4.3 回归问题评估</a></li></ul></section></li><li><section class="sidebar-group depth-0"><a href="/chapter5/index" class="sidebar-heading clickable"><span>第五章：机器学习实战</span> <!----></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/chapter5/spam_classify.html" class="sidebar-link">5.1 垃圾邮箱分类项目</a></li><li><a href="/chapter5/boston_house_price_forecast.html" class="sidebar-link">5.2 波士顿房价预测项目</a></li><li><a href="/chapter5/stock_price_predict.html" class="sidebar-link">5.3 股价预测项目</a></li><li><a href="/chapter5/card_anti_fraud.html" class="sidebar-link">5.4 信用卡反欺诈项目信用</a></li><li><a href="/chapter5/movie_recommend.html" class="sidebar-link">5.5 电影推荐系统</a></li></ul></section></li><li><a href="/sponsor.html" class="sidebar-link">赞助</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="_3-2-决策树算法"><a href="#_3-2-决策树算法" class="header-anchor">#</a> 3.2 决策树算法</h1> <h2 id="_3-2-1-决策树概念"><a href="#_3-2-1-决策树概念" class="header-anchor">#</a> 3.2.1 决策树概念</h2> <p>决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法</p> <p>决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树。</p> <p>怎么理解这句话？通过一个对话例子</p> <p><img src="/assets/img/3-2-1.eb6b12bc.png" alt="图3-2-1"></p> <p>想一想这个女生为什么把年龄放在最上面判断！</p> <p>上面案例是女生通过定性的主观意识，把年龄放到最上面，那么如果需要对这一过程进行量化，该如何处理呢？</p> <p>此时需要用到信息论中的知识：信息熵，信息增益</p> <h2 id="_3-2-2-决策树分类原理"><a href="#_3-2-2-决策树分类原理" class="header-anchor">#</a> 3.2.2 决策树分类原理</h2> <h3 id="_1-熵概念"><a href="#_1-熵概念" class="header-anchor">#</a> 1. 熵概念</h3> <p>物理学上，熵 Entropy 是“混乱”程度的量度。</p> <p>系统越有序，熵值越低；系统越混乱或者分散，熵值越高。</p> <ol><li>从信息完整性描述</li></ol> <p>当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。</p> <ol start="2"><li>从信息有序性描述</li></ol> <p>当数据量一致时，系统越有序，熵值越低；系统越混乱或者分散，熵值越高。</p> <p>假如事件A的分类划分是（A1,A2,...,An），每部分发生的概率是(p1,p2,...,pn)，那信息熵定义为公式如下：（log是以2为底）</p> <p><img src="/assets/img/3-2-2.ddbd1b85.png" alt="图3-2-2"></p> <h3 id="_2-熵案例介绍"><a href="#_2-熵案例介绍" class="header-anchor">#</a> 2. 熵案例介绍</h3> <ol><li>如果一颗骰子的六个面都是1 ，投掷它不会给你带来任何新信息，因为你知道它的结果肯定是1，它的信息熵为？</li></ol> <p>答案：log(1) = 0</p> <ol start="2"><li>假设我们没有看世界杯的比赛，但是想知道哪支球队会是冠军，我们只能猜测某支球队是或不是冠军，然后观众用对或不对来回答，我们想要猜测次数尽可能少，你会用什么方法？</li></ol> <p>答案：二分法<br>
计算过程：<br>
假如有 16 支球队，分别编号，先问是否在 1-8 之间，如果是就继续问是否在 1-4 之间，<br>
以此类推，直到最后判断出冠军球队是哪只。<br>
如果球队数量是 16，我们需要问 4 次来得到最后的答案。那么世界冠军这条消息的信息熵就是 4。</p> <p>如果有32个球队，准确的信息量应该是：<br>
H = -（p1 * logp1 + p2 * logp2 + ... + p32 * logp32），<br>
其中 p1, ..., p32 分别是这 32 支球队夺冠的概率。<br>
当每支球队夺冠概率相等都是 1/32 的时：H = -（32 * 1/32 * log1/32） = 5<br>
每个事件概率相同时，熵最大，这件事越不确定。</p> <h3 id="_3-信息增益概念"><a href="#_3-信息增益概念" class="header-anchor">#</a> 3. 信息增益概念</h3> <p>以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。</p> <p>特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：</p> <p><img src="/assets/img/3-2-3.976068c9.png" alt="图3-2-3"></p> <h3 id="_3-信息增益案例"><a href="#_3-信息增益案例" class="header-anchor">#</a> 3. 信息增益案例</h3> <p>如下左图，第一列为论坛号码，第二列为性别，第三列为活跃度，最后一列用户是否流失。</p> <p>我们要解决一个问题：性别和活跃度两个特征，哪个对用户流失影响更大？</p> <p><img src="/assets/img/3-2-4.0433e3b2.png" alt="图3-2-4"></p> <p>通过计算信息增益可以解决这个问题，统计上右表信息</p> <p>其中Positive为正样本（已流失），Negative为负样本（未流失），下面的数值为不同划分下对应的人数。</p> <p>可得到三个熵：</p> <p>整体熵：</p> <p><img src="/assets/img/3-2-5.7c36eb62.png" alt="图3-2-5"></p> <p>性别熵：</p> <p><img src="/assets/img/3-2-6.b2209332.png" alt="图3-2-6"></p> <p>活跃度熵：</p> <p><img src="data:image/png;base64,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" alt="图3-2-7"></p> <p>计算信息增益看指标影响：</p> <p>性别信息增益：</p> <p><img src="/assets/img/3-2-8.f78bd947.png" alt="图3-2-8"></p> <p>活跃度信息增益：</p> <p><img src="/assets/img/3-2-9.a99efe33.png" alt="图3-2-9"></p> <p>活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。</p> <p>在做特征选择或者数据分析的时候，我们应该重点考察活跃度这个指标。</p> <h3 id="_4-信息增益率"><a href="#_4-信息增益率" class="header-anchor">#</a> 4. 信息增益率</h3> <p>增益率：增益比率度量是用前面的增益度量Gain(S，A)和所分离信息度量SplitInformation(如上例的性别，活跃度等)的比值来共同定义的。</p> <p><img src="/assets/img/3-2-10.52d453ab.png" alt="图3-2-10"></p> <h3 id="_5-基尼值和基尼指数"><a href="#_5-基尼值和基尼指数" class="header-anchor">#</a> 5. 基尼值和基尼指数</h3> <p>基尼值Gini（D）：从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。</p> <p><img src="/assets/img/3-2-11.9951c52b.png" alt="图3-2-11"></p> <p>基尼指数Gini_index（D）：一般，选择使划分后基尼系数最小的属性作为最优化分属性。</p> <p><img src="/assets/img/3-2-12.1a99e969.png" alt="图3-2-12"></p> <h3 id="_6-基尼指数案例"><a href="#_6-基尼指数案例" class="header-anchor">#</a> 6. 基尼指数案例</h3> <p><img src="/assets/img/3-2-13.5ea4093a.png" alt="图3-2-13"></p> <ol><li>对数据集非类标号属性 {是否有房，婚姻状况，年收入} 分别计算它们的Gini系数增益，取Gini系数增益值最大的属性作为决策树的根节点属性。</li> <li>根节点的Gini系数为：</li></ol> <p><img src="/assets/img/3-2-14.04057458.png" alt="图3-2-14"></p> <ol start="3"><li><p>当根据是否有房来进行划分时，Gini系数增益计算过程为：
<img src="/assets/img/3-2-15.65ac1df4.png" alt="图3-2-15"> <img src="/assets/img/3-2-16.a2bfdffe.png" alt="图3-2-16"> <strong>由此可见，有房的人拖欠贷款概率小</strong></p></li> <li><p>若按婚姻状况属性来划分，属性婚姻状况有三个可能的取值{married，single，divorced}，分别计算划分后的Gini系数增益。</p></li></ol> <p>{married} | {single,divorced}</p> <p>{single} | {married,divorced}</p> <p>{divorced} | {single,married}</p> <p>分组为{married} | {single,divorced}时：</p> <p><img src="/assets/img/3-2-17.e27d072b.png" alt="图3-2-17"></p> <p>当分组为{single} | {married,divorced}时：</p> <p><img src="/assets/img/3-2-18.b156deb2.png" alt="图3-2-18"></p> <p>当分组为{divorced} | {single,married}时：</p> <p><img src="/assets/img/3-2-19.49296c19.png" alt="图3-2-19"></p> <p>对比计算结果，根据婚姻状况属性来划分根节点时取Gini系数增益最大的分组作为划分结果，即:{married} | {single,divorced}</p> <ol start="5"><li>同理可得年收入Gini：
对于年收入属性为数值型属性，首先需要对数据按升序排序，然后从小到大依次用相邻值的中间值作为分隔将样本划分为两组。例如当面对年收入为60和70这两个值时，我们算得其中间值为65。以中间值65作为分割点求出Gini系数增益。</li></ol> <p><img src="/assets/img/3-2-20.dc5c906c.png" alt="图3-2-20"></p> <p><img src="/assets/img/3-2-21.90bb426e.png" alt="图3-2-21"></p> <p><img src="/assets/img/3-2-22.30768dfa.png" alt="图3-2-22"></p> <p>最大化增益等价于最小化子女结点的不纯性度量（Gini系数）的加权平均值，现在我们希望最大化Gini系数的增益。根据计算知道，三个属性划分根节点的增益最大的有两个：年收入属性和婚姻状况，他们的增益都为0.12。此时，选取婚姻状况的属性作为第一次划分。</p> <ol start="6"><li>采用同样的方法，分别计算剩下属性，其中根节点的Gini系数为（此时是否拖欠贷款的各有3个records）</li></ol> <p><img src="/assets/img/3-2-23.f7e8b93c.png" alt="图3-2-23"></p> <ol start="7"><li>对于是否有房属性，可得：</li></ol> <p><img src="/assets/img/3-2-24.6afef47f.png" alt="图3-2-24"></p> <ol start="8"><li>对于年收入属性则有：</li></ol> <p><img src="/assets/img/3-2-25.42b104bb.png" alt="图3-2-25"></p> <p>是否有房分割出去后，剩余四个值再做Gini系数增益计算，收入的相邻值中点等于77.5的Gini系数增益最高，作为划分条件。</p> <p>最终决策树诞生，此时有新的数据来，我们可以直接按照这棵决策树的判断条件进行预测。</p> <p><img src="/assets/img/3-2-26.f432ba44.png" alt="图3-2-26"></p> <p>基尼值越纯的节点，越容易决策出结果。</p> <p>本章一开始的举例，把年龄放到最上面，是因为年龄这个属性，大概率决定了决策方向，如果太大直接结束。</p> <h2 id="_3-2-3-决策树构建总结"><a href="#_3-2-3-决策树构建总结" class="header-anchor">#</a> 3.2.3 决策树构建总结</h2> <h3 id="_1-构建决策树步骤"><a href="#_1-构建决策树步骤" class="header-anchor">#</a> 1. 构建决策树步骤</h3> <ol><li>开始将所有记录看作一个节点</li> <li>遍历每个变量的每一种分割方式，找到最好的分割点</li> <li>分割成两个节点N1和N2</li> <li>对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止。</li></ol> <h3 id="_2-决策树变量"><a href="#_2-决策树变量" class="header-anchor">#</a> 2. 决策树变量</h3> <ol><li><p>数字型（Numeric）：变量类型是整数或浮点数，如前面例子中的“年收入”。用“&gt;=”，“&gt;”,“&lt;”或“&lt;=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。</p></li> <li><p>名称型（Nominal）：类似编程语言中的枚举类型，变量只能从有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使用“=”来分割。</p></li></ol> <h3 id="_3-评估分割点的好坏"><a href="#_3-评估分割点的好坏" class="header-anchor">#</a> 3. 评估分割点的好坏</h3> <p>如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分割点。</p> <p>比如上面的例子，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，非常“纯”；“否”的节点，可以偿还贷款和无法偿还贷款的人都有，不是很“纯”，但是两个节点加起来的纯度之和与原始节点的纯度之差最大，所以按照这种方法分割。构建决策树采用贪心算法，只考虑当前纯度差最大的情况作为分割点。</p> <h2 id="_3-2-4-常见决策树类型比较"><a href="#_3-2-4-常见决策树类型比较" class="header-anchor">#</a> 3.2.4 常见决策树类型比较</h2> <p><img src="/assets/img/3-2-27.f44c2395.png" alt="图3-2-27"> <img src="/assets/img/3-2-28.ea7219a8.png" alt="图3-2-28"></p> <h3 id="_1-id3-算法"><a href="#_1-id3-算法" class="header-anchor">#</a> 1. ID3 算法</h3> <p>存在的缺点</p> <ol><li>ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。</li> <li>ID3算法只能对描述属性为离散型属性的数据集构造决策树。</li></ol> <h3 id="_2-c4-5算法"><a href="#_2-c4-5算法" class="header-anchor">#</a> 2. C4.5算法</h3> <h4 id="_1-做出的改进-为什么使用c4-5要好"><a href="#_1-做出的改进-为什么使用c4-5要好" class="header-anchor">#</a> 1. 做出的改进(为什么使用C4.5要好)</h4> <ol><li>用信息增益率来选择属性</li> <li>可以处理连续数值型属性</li> <li>采用了一种后剪枝方法</li> <li>对于缺失值的处理</li></ol> <h4 id="_2-c4-5算法的优缺点"><a href="#_2-c4-5算法的优缺点" class="header-anchor">#</a> 2. C4.5算法的优缺点</h4> <p>优点：</p> <p>产生的分类规则易于理解，准确率较高。</p> <p>缺点：</p> <p>在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。
此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p> <h3 id="_3-cart算法"><a href="#_3-cart算法" class="header-anchor">#</a> 3. CART算法</h3> <p>CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算。</p> <p>C4.5不一定是二叉树，但CART一定是二叉树。</p> <p>同时，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。</p> <p>如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过随机森林之类的方法解决。</p> <p>掌握了信息熵、信息增益、信息增益率、基尼系数、基尼系数增益的计算方法，接下来就可以开始构造决策树了。</p> <h2 id="_3-2-5-cart剪枝"><a href="#_3-2-5-cart剪枝" class="header-anchor">#</a> 3.2.5 cart剪枝</h2> <h3 id="_1-为什么要剪枝"><a href="#_1-为什么要剪枝" class="header-anchor">#</a> 1. 为什么要剪枝</h3> <p><img src="/assets/img/3-2-29.0af2355c.png" alt="图3-2-29"></p> <ul><li>横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。</li> <li>实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。</li> <li>随着树的增长，在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。</li></ul> <h4 id="出现这种情况的原因"><a href="#出现这种情况的原因" class="header-anchor">#</a> 出现这种情况的原因：</h4> <ol><li>噪声、样本冲突，即错误的样本数据。</li> <li>特征即属性不能完全作为分类标准。</li> <li>巧合的规律性，数据量不够大。</li></ol> <h3 id="_2-常用的减枝方法"><a href="#_2-常用的减枝方法" class="header-anchor">#</a> 2. 常用的减枝方法</h3> <h4 id="_1-预剪枝"><a href="#_1-预剪枝" class="header-anchor">#</a> 1. 预剪枝</h4> <ol><li>每一个结点所包含的最小样本数目，例如10，则该结点总样本数小于10时，则不再分；</li> <li>指定树的高度或者深度，例如树的最大深度为4；</li> <li>指定结点的熵小于某个值，不再划分。随着树的增长，在训练样集上的精度是单调上升的，然而在独立的测试样例上测出的精度先上升后下降。</li></ol> <h4 id="_2-后剪枝"><a href="#_2-后剪枝" class="header-anchor">#</a> 2. 后剪枝：</h4> <p>在已生成<code>过拟合</code>决策树上进行剪枝，可以得到简化版的剪枝决策树。</p> <h2 id="_2-2-6-决策树算法api"><a href="#_2-2-6-决策树算法api" class="header-anchor">#</a> 2.2.6 决策树算法API</h2> <h3 id="_1-构建参数介绍"><a href="#_1-构建参数介绍" class="header-anchor">#</a> 1. 构建参数介绍</h3> <div class="language-py extra-class"><pre class="language-py"><code>sklearn<span class="token punctuation">.</span>tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">'gini'</span><span class="token punctuation">,</span> max_depth<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre></div><ul><li><p>criterion</p> <ul><li>特征选择标准</li> <li>&quot;gini&quot;或者&quot;entropy&quot;，前者代表基尼系数，后者代表信息增益。一默认&quot;gini&quot;，即CART算法。</li></ul></li> <li><p>min_samples_split</p> <ul><li>内部节点再划分所需最小样本数</li> <li>这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。</li></ul></li> <li><p>min_samples_leaf</p> <ul><li>叶子节点最少样本数</li> <li>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</li></ul></li> <li><p>max_depth</p> <ul><li>决策树最大深度</li> <li>决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间</li></ul></li> <li><p>random_state</p> <ul><li>随机数种子</li></ul></li></ul> <h3 id="_2-代码过程"><a href="#_2-代码过程" class="header-anchor">#</a> 2. 代码过程</h3> <h4 id="_1-导入依赖"><a href="#_1-导入依赖" class="header-anchor">#</a> 1. 导入依赖</h4> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> joblib
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> plot_tree
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
</code></pre></div><h4 id="_2-数据集介绍"><a href="#_2-数据集介绍" class="header-anchor">#</a> 2. 数据集介绍</h4> <p><strong>泰坦尼克号数据</strong></p> <p>在泰坦尼克号和titanic2数据帧描述泰坦尼克号上的个别乘客的生存状态。这里使用的数据集是由各种研究人员开始的。其中包括许多研究人员创建的旅客名单，由Michael A. Findlay编辑。我们提取的数据集中的特征是票的类别，存活，乘坐班，年龄，登陆，home.dest，房间，票，船和性别。</p> <blockquote><p>数据：https://www.kaggle.com/competitions/titanic/data?select=train.csv</p></blockquote> <p><img src="/assets/img/3-2-30.0544e444.png" alt="图3-2-30"></p> <p>经过观察数据得到：</p> <ul><li>乘坐班是指乘客班（1，2，3），是社会经济阶层的代表。</li> <li>其中age数据存在缺失。</li></ul> <h4 id="_3-数据处理"><a href="#_3-数据处理" class="header-anchor">#</a> 3. 数据处理</h4> <ol><li>获取数据</li></ol> <div class="language-py extra-class"><pre class="language-py"><code>    titan_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">&quot;./data/泰坦尼克号数据集.csv&quot;</span><span class="token punctuation">)</span>
</code></pre></div><ol start="2"><li>数据基本处理</li></ol> <ul><li>确定特征值，目标值</li></ul> <div class="language-py extra-class"><pre class="language-py"><code>x <span class="token operator">=</span> titan<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">&quot;Pclass&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;Age&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;Sex&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
y <span class="token operator">=</span> titan<span class="token punctuation">[</span><span class="token string">&quot;Survived&quot;</span><span class="token punctuation">]</span>
</code></pre></div><ul><li>缺失值处理</li></ul> <div class="language-py extra-class"><pre class="language-py"><code><span class="token comment"># 缺失值需要处理，将特征当中有类别的这些特征进行字典特征抽取</span>
x<span class="token punctuation">[</span><span class="token string">'Age'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token string">'Age'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre></div><ul><li>数据划分训练集、测试集</li></ul> <div class="language-py extra-class"><pre class="language-py"><code>x_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre></div><h4 id="_4-特征工程"><a href="#_4-特征工程" class="header-anchor">#</a> 4. 特征工程</h4> <p>关于特征工程详情介绍：<a href="/chapter4/feature_engineering.html">特征工程</a></p> <p>特征中出现类别符号，需要进行one-hot编码处理</p> <div class="language-py extra-class"><pre class="language-py"><code><span class="token comment"># 对类别特征进行one-hot编码</span>
x_train <span class="token operator">=</span> pd<span class="token punctuation">.</span>get_dummies<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Pclass'</span><span class="token punctuation">,</span> <span class="token string">'Sex'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
x_test <span class="token operator">=</span> pd<span class="token punctuation">.</span>get_dummies<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Pclass'</span><span class="token punctuation">,</span> <span class="token string">'Sex'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><h4 id="_5-模型训练"><a href="#_5-模型训练" class="header-anchor">#</a> 5. 模型训练</h4> <p>如果没有指定max_depth那么会根据信息熵的条件直到最终结束。这里我们可以指定树的深度来进行限制树的大小</p> <div class="language-py extra-class"><pre class="language-py"><code><span class="token comment"># 模型训练</span>
model <span class="token operator">=</span> DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">&quot;entropy&quot;</span><span class="token punctuation">,</span> max_depth<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

<span class="token comment"># 模型评估</span>
score <span class="token operator">=</span> model<span class="token punctuation">.</span>score<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;模型评估得分：&quot;</span><span class="token punctuation">,</span> score<span class="token punctuation">)</span>
<span class="token comment"># 预测结果</span>
res_y <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;预测结果：&quot;</span><span class="token punctuation">,</span> res_y<span class="token punctuation">)</span>
</code></pre></div><h4 id="_6-模型保存"><a href="#_6-模型保存" class="header-anchor">#</a> 6. 模型保存</h4> <p>训练好的模型持久化硬盘存储，下次使用直接加载，无需重复训练</p> <div class="language-py extra-class"><pre class="language-py"><code>joblib<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'model/dt.pth'</span><span class="token punctuation">)</span>
</code></pre></div><h2 id="_2-2-7-决策树可视化"><a href="#_2-2-7-决策树可视化" class="header-anchor">#</a> 2.2.7 决策树可视化</h2> <h3 id="_1-加载模型"><a href="#_1-加载模型" class="header-anchor">#</a> 1. 加载模型</h3> <div class="language-py extra-class"><pre class="language-py"><code>estimator <span class="token operator">=</span> joblib<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model/dt.pth'</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="_2-生成可视化图片"><a href="#_2-生成可视化图片" class="header-anchor">#</a> 2. 生成可视化图片</h3> <div class="language-py extra-class"><pre class="language-py"><code>fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plot_tree<span class="token punctuation">(</span>estimator<span class="token punctuation">,</span>
          ax<span class="token operator">=</span>ax<span class="token punctuation">,</span>
          max_depth<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
          filled<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
          feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Age'</span><span class="token punctuation">,</span> <span class="token string">'Pclass_1'</span><span class="token punctuation">,</span> <span class="token string">'Pclass_2'</span><span class="token punctuation">,</span> <span class="token string">'Pclass_3'</span><span class="token punctuation">,</span> <span class="token string">'Sex_female'</span><span class="token punctuation">,</span> <span class="token string">'Sex_male'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'生存'</span><span class="token punctuation">,</span> <span class="token string">'不生存'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'output/a.png'</span><span class="token punctuation">,</span> dpi<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
</code></pre></div><h2 id="_2-2-8-决策树算法优缺点"><a href="#_2-2-8-决策树算法优缺点" class="header-anchor">#</a> 2.2.8 决策树算法优缺点</h2> <ul><li>优点：
<ul><li>简单的理解和解释，树木可视化。</li></ul></li> <li>缺点：
<ul><li>决策树学习者可以创建不能很好地推广数据的过于复杂的树,容易发生过拟合。</li></ul></li> <li>改进：
<ul><li>减枝cart算法</li> <li>随机森林（集成学习的一种）</li></ul></li></ul></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">11/25/2023, 8:18:06 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/chapter3/knn.html" class="prev">
        3.1 K-近邻算法介绍
      </a></span> <span class="next"><a href="/chapter3/naive_bayes.html">
        3.3 朴素贝叶斯算法
      </a>
      →
    </span></p></div> <div style="text-align:center;"></div> <div class="copyright"><a target="_blank" href="https://beian.miit.gov.cn">豫ICP备2023030173号-1</a></div> <div class="copyright"> 
        版权所有，禁止私自克隆网站。
      </div></main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.d8974754.js" defer></script><script src="/assets/js/4.c4ff76d6.js" defer></script><script src="/assets/js/1.bfbdf300.js" defer></script><script src="/assets/js/11.a789a3a6.js" defer></script>
  </body>
</html>
